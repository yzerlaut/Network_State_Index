{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9476069",
   "metadata": {},
   "source": [
    "# Network State Index -- API demo\n",
    "\n",
    "see: https://github.com/yzerlaut/Network_State_Index\n",
    "\n",
    "We demonstrate the use of the API on the following publicly available dataset:\n",
    "\n",
    "## the \"Visual Coding â€“ Neuropixels\" dataset from the Allen Observatory\n",
    "\n",
    "All details about this dataset and instructions for analysis are available at:\n",
    "\n",
    "https://allensdk.readthedocs.io/en/latest/visual_coding_neuropixels.html\n",
    "\n",
    "## Dataset download\n",
    "\n",
    "I made a [custom script](https://github.com/yzerlaut/Network_State_Index/blob/main/demo/download_Allen_Visual-Coding_dataset.py) to download exclusively the part of the dataset of interest here (V1 probes).\n",
    "You can run the script as:\n",
    "```\n",
    "python demo/download_Allen_Visual-Coding_dataset.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392f4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some general python / scientific-python modules\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# the Network State Index API, see: https://github.com/yzerlaut/Network_State_Index\n",
    "# install it with: \"pip install git+https://github.com/yzerlaut/Network_State_Index\"\n",
    "import nsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f38353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the data with the \"allensdk\" API\n",
    "# get the \"allensdk\" api with: \"pip install allensdk\"\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "\n",
    "# now let's define a cache repository for the data: by default ~/Downloads/ecephys_cache_dir\n",
    "# insure that you have a \"Downloads\" repository in your home directory (/!\\ non-english systems) or update below\n",
    "data_directory = os.path.join(os.path.expanduser('~'), 'Downloads', 'ecephys_cache_dir')\n",
    "manifest_path = os.path.join(data_directory, \"manifest.json\")\n",
    "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
    "all_sessions = cache.get_session_table() # get all sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68248afc",
   "metadata": {},
   "source": [
    "### We restrict the analysis to:\n",
    "\n",
    "    - wild type / wild type strain\n",
    "    - male\n",
    "    - recordings including V1 (\"VISp\")\n",
    "    - \"Brain Observatory 1.1\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2cb729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "--> Number of sessions with the desired characteristics: 11------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's filter the sessions according to the above criteria\n",
    "sessions = all_sessions[(all_sessions.sex == 'M') & \\\n",
    "                        (all_sessions.full_genotype.str.find('wt/wt') > -1) & \\\n",
    "                        #(all_sessions.session_type == 'brain_observatory_1.1') & \\\n",
    "                        (all_sessions.session_type == 'functional_connectivity') & \\\n",
    "                        (['VISp' in acronyms for acronyms in all_sessions.ecephys_structure_acronyms])]\n",
    "print(30*'--'+'\\n--> Number of sessions with the desired characteristics: ' + str(len(sessions))+30*'--')\n",
    "# sessions.head() # uncomment to see how they look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389ff9a",
   "metadata": {},
   "source": [
    "## Minimal demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFP = np.random.randn(100)\n",
    "pLFP = NSI.compute_pLFP(LFP,\n",
    "                       freqs=np.linspace(50, 300, 10))\n",
    "NSI = NSI.compute_NSI(pLFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cadf5",
   "metadata": {},
   "source": [
    "## In a more structured way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do things in a bit more structured way\n",
    "import time\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    an object to load, format and process the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, session_index,\n",
    "                 t0=120*60, \n",
    "                 duration=1*60, # 1 min by defualt\n",
    "                 #init=['raster', 'pop_act', 'pLFP', 'NSI'], # for a full init\n",
    "                 init = []):\n",
    "        \"\"\"\n",
    "        loading data according to the index of the \"sessions\" above\n",
    "        \n",
    "        can load a subset using the \"t0\" and \"duration\" args\n",
    "        \"\"\"\n",
    "        print('loading session #%i [...]' % (1+session_index))\n",
    "        tic = time.time()\n",
    "        # we load a single session\n",
    "        session = cache.get_session_data(sessions.index.values[session_index])\n",
    "\n",
    "        # use the running timestamps to set start and duration in the data object\n",
    "        self.t0 = np.max([t0, session.running_speed.start_time.values[0]])\n",
    "        self.duration = np.min([duration, session.running_speed.end_time.values[-1]-self.t0])\n",
    "        \n",
    "        # let's fetch the running speed\n",
    "        cond = (session.running_speed.end_time.values>self.t0) &\\\n",
    "            (session.running_speed.start_time.values<(self.t0+self.duration))\n",
    "        self.t_running_speed = .5*(session.running_speed.start_time.values[cond]+\\\n",
    "                                   session.running_speed.end_time.values[cond])\n",
    "        self.running_speed = session.running_speed.velocity[cond]\n",
    "\n",
    "        if 'raster' in init:\n",
    "            # let's fetch the isolated single units in V1\n",
    "            V1_units = session.units[session.units.ecephys_structure_acronym == 'VISp'] # V1==VISp\n",
    "            self.V1_RASTER = []\n",
    "            for i in V1_units.index:\n",
    "                cond = (session.spike_times[i]>=self.t0) & (session.spike_times[i]<(self.t0+self.duration))\n",
    "                self.V1_RASTER.append(session.spike_times[i][cond])\n",
    "        else:\n",
    "            self.V1_RASTER = None\n",
    "        \n",
    "        # let's fetch the V1 probe --> always on \"probeC\"\n",
    "        probe_id = session.probes[session.probes.description == 'probeC'].index.values[0]\n",
    "        \n",
    "        # -- let's fetch the lfp data for that probe and that session --\n",
    "        # let's fetch the all the channels falling into V1 domain\n",
    "        self.V1_channel_ids = session.channels[(session.channels.probe_id == probe_id) & \\\n",
    "                      (session.channels.ecephys_structure_acronym.isin(['VISp']))].index.values\n",
    "\n",
    "        # limit LFP to desired times and channels\n",
    "        # N.B. \"get_lfp\" returns a subset of all channels above\n",
    "        self.lfp_slice_V1 = session.get_lfp(probe_id).sel(time=slice(self.t0,\n",
    "                                                                     self.t0+self.duration),\n",
    "                                                          channel=slice(np.min(self.V1_channel_ids), \n",
    "                                                                        np.max(self.V1_channel_ids)))\n",
    "        self.Nchannels_V1 = len(self.lfp_slice_V1.channel) # store number of channels with LFP in V1\n",
    "        self.lfp_sampling_rate = session.probes.lfp_sampling_rate[probe_id] # keeping track of sampling rate\n",
    "        \n",
    "        \n",
    "        if 'pop_act' in init:\n",
    "            # let's compute the population activity from the spikes\n",
    "            self.compute_pop_act() # you can recall this functions with different bins/smoothing        \n",
    "            \n",
    "        if 'pLFP' in init:\n",
    "            self.compute_pLFP(t0=self.t0, duration=self.duration) # on the full trace\n",
    "            if 'NSI' in init:\n",
    "                self.compute_NSI()\n",
    "                \n",
    "        print('data successfully loaded in %.1fs' % (time.time()-tic))\n",
    "\n",
    "    def update_t0_duration(self, t0, duration):\n",
    "        t0 = t0 if (t0 is not None) else self.t0\n",
    "        duration = duration if (duration is not None) else self.duration\n",
    "        return t0, duration\n",
    "        \n",
    "    def compute_pop_act(self, \n",
    "                        pop_act_bin=5e-3,\n",
    "                        pop_act_smoothing=20e-3):\n",
    "        \"\"\"\n",
    "        we bin spikes to compute population activity\n",
    "        \"\"\"\n",
    "        t_pop_act = self.t0+np.arange(int(self.duration/pop_act_bin)+1)*pop_act_bin\n",
    "        pop_act = np.zeros(len(t_pop_act)-1)\n",
    "\n",
    "        for i, spikes in enumerate(self.V1_RASTER):\n",
    "            pop_act += np.histogram(spikes, bins=t_pop_act)[0]\n",
    "        pop_act /= (len(self.V1_RASTER)*pop_act_bin)\n",
    "\n",
    "        self.t_pop_act = .5*(t_pop_act[1:]+t_pop_act[:-1])\n",
    "        self.pop_act = nsi.gaussian_filter1d(pop_act, \n",
    "                                             int(pop_act_smoothing/pop_act_bin)) # filter from scipy\n",
    "        self.pop_act_sampling_rate = pop_act_bin\n",
    "        \n",
    "    def get_LFP(self, channelID,\n",
    "                ground_channel_ID=0,\n",
    "                t0=None, \n",
    "                duration=None):\n",
    "        \"\"\"\n",
    "        channelID can be an integer or a list of integers\n",
    "        \n",
    "        returns time in s, LFP in mV\n",
    "        \"\"\"\n",
    "        t0, duration = self.update_t0_duration(t0, duration)\n",
    "        if type(channelID) in [range, list, np.array]:\n",
    "            self.LFP = 1e3*self.lfp_slice_V1.isel(channel=channelID[0]).sel(time=slice(t0,t0+duration)).values/len(channelID)\n",
    "            self.t_LFP = self.lfp_slice_V1.isel(channel=channelID[0]).sel(time=slice(t0,t0+duration)).time.values\n",
    "            for i in channelID[1:]:\n",
    "                self.LFP += 1e3*self.lfp_slice_V1.isel(channel=i).sel(time=slice(t0,t0+duration)).values/len(channelID)\n",
    "        else:\n",
    "            self.LFP = 1e3*self.lfp_slice_V1.isel(channel=channelID).sel(time=slice(t0,t0+duration)).values\n",
    "            self.t_LFP = self.lfp_slice_V1.isel(channel=channelID).sel(time=slice(t0,t0+duration)).time.values\n",
    "            \n",
    "        if ground_channel_ID is not None:\n",
    "            self.LFP -= 1e3*self.lfp_slice_V1.isel(channel=ground_channel_ID).sel(time=slice(t0,t0+duration)).values\n",
    "            \n",
    "        return self.t_LFP, self.LFP\n",
    "    \n",
    "    def compute_pLFP(self, \n",
    "                     channelIDs=None, ground_channel_ID=0,\n",
    "                     t0=None, duration=None,\n",
    "                     freqs = np.linspace(72.8/1.83,72.8*1.83,20),\n",
    "                     new_dt=2e-3,\n",
    "                     smoothing=42e-3):\n",
    "        \"\"\"\n",
    "        ------------------------------\n",
    "            HERE we use the NSI API\n",
    "        ------------------------------\n",
    "        by default on a zoom of the data, not the entire trace as this can be long\n",
    "        call: \"data.compute_pLFP(duration=data.duration)\" to have it on the full trace\n",
    "        \n",
    "        pLFP in microvolts (uV)\n",
    "        \"\"\"\n",
    "        print(' - computing pLFP [...]') \n",
    "        self.pLFP_sampling_rate = 1./new_dt\n",
    "        \n",
    "        t0, duration = self.update_t0_duration(t0, duration)\n",
    "        \n",
    "        if channelIDs is None:\n",
    "            channelIDs = np.arange(self.Nchannels_V1) # using all channels\n",
    "          \n",
    "        # we use the first channel to init the pLFP\n",
    "        self.t_pLFP, self.pLFP = nsi.compute_pLFP(1e3*self.get_LFP(channelID=channelIDs[0],\n",
    "                                                               ground_channel_ID=ground_channel_ID,\n",
    "                                                               t0=t0, \n",
    "                                                               duration=duration)[1]/len(channelIDs),\n",
    "                                                  self.lfp_sampling_rate,\n",
    "                                                  freqs=freqs, \n",
    "                                                  new_dt=new_dt,\n",
    "                                                  smoothing=smoothing)\n",
    "        self.t_pLFP += t0\n",
    "        \n",
    "        # we loop over all other channels (in case they exist)\n",
    "        for i in channelIDs[1:]:\n",
    "            self.pLFP += 1e3*nsi.compute_pLFP(self.get_LFP(channelID=i,\n",
    "                                                       ground_channel_ID=ground_channel_ID,\n",
    "                                                       t0=t0, duration=duration)[1],\n",
    "                                          self.lfp_sampling_rate,\n",
    "                                          freqs=freqs, \n",
    "                                          new_dt=new_dt,\n",
    "                                          smoothing=smoothing)[1]/len(channelIDs)\n",
    "        \n",
    "        print(' - - > done !') \n",
    "        \n",
    "        \n",
    "    def compute_NSI(self, quantity='pLFP',\n",
    "                    low_freqs = np.linspace(2, 5, 5),\n",
    "                    p0_percentile=1.,\n",
    "                    alpha=2.87,\n",
    "                    with_subquantities=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        setattr(self, '%s_0' % quantity, np.percentile(getattr(self, quantity), p0_percentile/100.))\n",
    "        \n",
    "        if with_subquantities:\n",
    "            lfe, sm, NSI = nsi.compute_NSI(getattr(self, quantity),\n",
    "                                           getattr(self, '%s_sampling_rate' % quantity),\n",
    "                                           low_freqs = low_freqs,\n",
    "                                           p0=getattr(self, '%s_0' % quantity),\n",
    "                                           alpha=alpha,\n",
    "                                           with_subquantities=True)\n",
    "            setattr(self, '%s_low_freq_env' % quantity, lfe)\n",
    "            setattr(self, '%s_sliding_mean' % quantity, sm)\n",
    "            setattr(self, '%s_NSI' % quantity, NSI)\n",
    "        \n",
    "        else:\n",
    "            setattr(self, '%s_NSI' % quantity, nsi.compute_NSI(getattr(self, quantity),\n",
    "                                                              getattr(self, '%s_sampling_rate' % quantity),\n",
    "                                                              low_freqs = low_freqs,\n",
    "                                                              p0=getattr(self, '%s_0' % quantity),\n",
    "                                                              alpha=alpha))\n",
    "        \n",
    "    def plot(self, quantity, \n",
    "             t0=None, duration=None,\n",
    "             ax=None,\n",
    "             subsampling=1,\n",
    "             color='k', \n",
    "             lw=1):\n",
    "        \"\"\"\n",
    "        quantity as a string (e.g. \"pLFP\" or \"running_speed\")\n",
    "        \"\"\"\n",
    "        \n",
    "        t0, duration = self.update_t0_duration(t0, duration)\n",
    "        \n",
    "        if hasattr(self, quantity):\n",
    "            if ax is None:\n",
    "                fig, ax =plt.subplots(1, figsize=(8,3))\n",
    "            else:\n",
    "                fig = None\n",
    "            t = getattr(self, 't_'+quantity.replace('_NSI','').replace('_low_freq_env','').replace('_sliding_mean',''))\n",
    "            signal = getattr(self, quantity)\n",
    "            cond = (t>t0) & (t<(t0+duration))\n",
    "            ax.plot(t[cond][::subsampling], signal[cond][::subsampling], color=color, lw=lw)\n",
    "            return fig, ax\n",
    "        else:\n",
    "            print('%s not an attribute of data' % quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(0, t0=110*60, duration=20*60, init=['raster', 'pop_act'])\n",
    "fig, ax = data.plot('running_speed')\n",
    "ax.set_ylabel('speed (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a82c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "def plot_sample_data(data,\n",
    "                     time_points=[100, 1000],\n",
    "                     duration=2., \n",
    "                     pop_act_bin=5e-3):\n",
    "\n",
    "    fig, AX_full = plt.subplots(6,len(time_points), figsize=(3*len(time_points), 7))\n",
    "    if len(time_points)==1:\n",
    "        AX_full = [AX_full]\n",
    "        \n",
    "    YLIMS = [[np.inf, -np.inf] for i in range(len(AX_full))]\n",
    "    for t0, AX in zip(time_points, AX_full.T):\n",
    "        \n",
    "        # raster plot\n",
    "        for i, spikes in enumerate(data.V1_RASTER):\n",
    "            cond = (spikes>t0) & (spikes<(t0+duration))\n",
    "            AX[0].plot(spikes[cond], i+0*spikes[cond], 'ko', ms=1.5)\n",
    "\n",
    "        # pop act. plot\n",
    "        data.plot('pop_act', t0=t0, duration=duration, ax=AX[1])\n",
    "        \n",
    "        # LFP plot\n",
    "        lfp_slice = data.lfp_slice_V1.sel(time=slice(t0,t0+duration))\n",
    "        for i in range(len(lfp_slice.channel.values)):\n",
    "            AX[2].plot(lfp_slice.time, 1e3*lfp_slice.sel(channel=lfp_slice.channel[i]), \n",
    "                       lw=0.2, color=plt.cm.copper(1-i/(len(lfp_slice.channel.values)-1)))\n",
    "        \n",
    "        # pLFP plot\n",
    "        data.plot('pLFP', t0=t0, duration=duration, ax=AX[3], color=plt.cm.tab10(5))\n",
    "\n",
    "        # NSI plot\n",
    "        data.plot('pLFP_NSI', t0=t0, duration=duration, ax=AX[4], color=plt.cm.tab10(5))\n",
    "\n",
    "        # speed plot\n",
    "        data.plot('running_speed', t0=t0, duration=duration, ax=AX[5])\n",
    "\n",
    "        # labelling axes and setting the same limes\n",
    "        for j, label, ax in zip(range(len(AX)), \n",
    "                                ['units', 'rate (Hz)', 'LFP (mV)', 'pLFP (uV)', 'NSI (uV)', 'run. speed\\n(cm/s)'],\n",
    "                                AX):\n",
    "            if ax in AX_full.T[0]:\n",
    "                ax.set_ylabel(label)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_xlim([t0,t0+duration])\n",
    "            YLIMS[j] = [np.min([AX[j].get_ylim()[0], YLIMS[j][0]]),\n",
    "                        np.max([AX[j].get_ylim()[1], YLIMS[j][1]])]\n",
    "        AX[0].set_title('$t_0$=%.1fs' % (t0), size='small')\n",
    "        \n",
    "    for AX in AX_full.T:\n",
    "        for j in range(len(AX)):\n",
    "            try:\n",
    "                AX[j].set_ylim(YLIMS[j])\n",
    "            except BaseException:\n",
    "                pass\n",
    "    for t0, AX in zip(time_points, AX_full.T):\n",
    "        AX[1].plot([t0,t0+0.2], .9*YLIMS[1][1]*np.ones(2), 'k-', lw=1)\n",
    "        AX[1].annotate('200ms', (t0, .92*YLIMS[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4014b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_data(data, \n",
    "                 time_points=[6605.5, 6602.3, 7245]) # session 1 -- functional_connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d54f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_data(data, time_points=[7199.5, 7531, 7421.5]) # session 1 - brain observatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "342ce772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([198.   , 198.005, 198.01 , ..., 212.565, 212.57 , 212.575])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking for a good grounding...\n",
    "\n",
    "#_ = data.plot('LFP[%i]' % channelID, t0=6605, duration=3)\n",
    "channelIDs, gcID = [0], 2\n",
    "data.get_LFP(channelIDs, ground_channel_ID=gcID)\n",
    "data.compute_pLFP(channelIDs=channelIDs,ground_channel_ID=gcID,t0=6605, duration=3)\n",
    "fig, ax = plt.subplots(1, figsize=(8,4))\n",
    "_ = data.plot('LFP', t0=6605, duration=3, ax=ax)\n",
    "_ = data.plot('pLFP', t0=6605, duration=3, ax=ax.twinx(),\n",
    "              color=plt.cm.tab10(3), lw=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
