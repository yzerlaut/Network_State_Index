{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9476069",
   "metadata": {},
   "source": [
    "# Network State Index -- API demo\n",
    "\n",
    "see: https://github.com/yzerlaut/Network_State_Index\n",
    "\n",
    "We demonstrate the use of the API on the following publicly available dataset:\n",
    "\n",
    "## the \"Visual Coding â€“ Neuropixels\" dataset from the Allen Observatory\n",
    "\n",
    "All details about this dataset and instructions for analysis are available at:\n",
    "\n",
    "https://allensdk.readthedocs.io/en/latest/visual_coding_neuropixels.html\n",
    "\n",
    "## Dataset download\n",
    "\n",
    "I made a [custom script](https://github.com/yzerlaut/Network_State_Index/blob/main/demo/download_Allen_Visual-Coding_dataset.py) to download exclusively the part of the dataset of interest here (V1 probes).\n",
    "You can run the script as:\n",
    "```\n",
    "python demo/download_Allen_Visual-Coding_dataset.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392f4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some general python / scientific-python modules\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# the Network State Index API, see: https://github.com/yzerlaut/Network_State_Index\n",
    "# install it with: \"pip install git+https://github.com/yzerlaut/Network_State_Index\"\n",
    "import nsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f38353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the data with the \"allensdk\" API\n",
    "# get the \"allensdk\" api with: \"pip install allensdk\"\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "\n",
    "# now let's define a cache repository for the data: by default ~/Downloads/ecephys_cache_dir\n",
    "# insure that you have a \"Downloads\" repository in your home directory (/!\\ non-english systems) or update below\n",
    "data_directory = os.path.join(os.path.expanduser('~'), 'Downloads', 'ecephys_cache_dir')\n",
    "manifest_path = os.path.join(data_directory, \"manifest.json\")\n",
    "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
    "all_sessions = cache.get_session_table() # get all sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68248afc",
   "metadata": {},
   "source": [
    "### We restrict the analysis to:\n",
    "\n",
    "    - wild type / wild type strain\n",
    "    - male\n",
    "    - recordings including V1 (\"VISp\")\n",
    "    - \"Brain Observatory 1.1\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2cb729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "--> Number of sessions with the desired characteristics: 11\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's filter the sessions according to the above criteria\n",
    "sessions = all_sessions[(all_sessions.sex == 'M') & \\\n",
    "                        (all_sessions.full_genotype.str.find('wt/wt') > -1) & \\\n",
    "                        #(all_sessions.session_type == 'brain_observatory_1.1') & \\\n",
    "                        (all_sessions.session_type == 'functional_connectivity') & \\\n",
    "                        (['VISp' in acronyms for acronyms in all_sessions.ecephys_structure_acronyms])]\n",
    "print(30*'--'+'\\n--> Number of sessions with the desired characteristics: ' + str(len(sessions))+'\\n'+30*'--')\n",
    "# sessions.head() # uncomment to see how they look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cadf5",
   "metadata": {},
   "source": [
    "## Loading, formatting and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d955ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    an object to load, format and process the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 session_index=0,\n",
    "                 demo=True, demo_filename='allen_demo_sample.npy',\n",
    "                 reduced=False,\n",
    "                 t0=115*60, \n",
    "                 duration=20*60, # 20 min by default\n",
    "                 #init=['pop_act', 'pLFP', 'NSI'], # for a full init\n",
    "                 init = []):\n",
    "        \"\"\"\n",
    "        loading data according to the index of the \"sessions\" above\n",
    "        to troubleshoot, there is a \"demo\" option that loads the sample provided in the repo\n",
    "        \n",
    "        can load a subset using the \"t0\" and \"duration\" args\n",
    "        \"\"\"\n",
    "        \n",
    "        if demo:\n",
    "            # -------- using the stored demo data if \"demo\" mode ----- #\n",
    "            DEMO = np.load(demo_filename, allow_pickle=True).item()\n",
    "            for key in DEMO:\n",
    "                setattr(self, key, DEMO[key]) # sets LFP\n",
    "            self.t_LFP = np.arange(len(self.LFP))/self.lfp_sampling_rate+self.t0\n",
    "        elif reduced:\n",
    "            rdata = np.load('reduced_data/Allen_FC_session%i.npy' % (session_index+1), allow_pickle=True).item()\n",
    "            for key in rdata:\n",
    "                setattr(self, key, rdata[key]) # sets LFP, pLFP, pop_act, running_speed\n",
    "            for key in ['LFP', 'pLFP', 'pop_act']:\n",
    "                setattr(self, 't_%s'%key, np.arange(len(getattr(self,key)))/getattr(self,'%s_sampling_rate'%key)+self.t0)\n",
    "        else:\n",
    "            # -------------------------------------------------------- # \n",
    "            # -- using the Allen SDK to retrieve and cache the data -- #\n",
    "            # -------------------------------------------------------- # \n",
    "\n",
    "            print('loading session #%i [...]' % (1+session_index))\n",
    "            tic = time.time()\n",
    "            # we load a single session\n",
    "            session = cache.get_session_data(sessions.index.values[session_index])\n",
    "\n",
    "            # use the running timestamps to set start and duration in the data object\n",
    "            self.t0 = np.max([t0, session.running_speed.start_time.values[0]])\n",
    "            self.duration = np.min([duration, session.running_speed.end_time.values[-1]-self.t0])\n",
    "\n",
    "            # let's fetch the running speed\n",
    "            cond = (session.running_speed.end_time.values>self.t0) &\\\n",
    "                (session.running_speed.start_time.values<(self.t0+self.duration))\n",
    "            self.t_running_speed = .5*(session.running_speed.start_time.values[cond]+\\\n",
    "                                       session.running_speed.end_time.values[cond])\n",
    "            self.running_speed = session.running_speed.velocity[cond]\n",
    "\n",
    "            # let's fetch the isolated single units in V1\n",
    "            V1_units = session.units[session.units.ecephys_structure_acronym == 'VISp'] # V1==VISp\n",
    "            self.V1_RASTER = []\n",
    "            for i in V1_units.index:\n",
    "                cond = (session.spike_times[i]>=self.t0) & (session.spike_times[i]<(self.t0+self.duration))\n",
    "                self.V1_RASTER.append(session.spike_times[i][cond])\n",
    "\n",
    "            # let's fetch the V1 probe --> always on \"probeC\"\n",
    "            probe_id = session.probes[session.probes.description == 'probeC'].index.values[0]\n",
    "\n",
    "            # -- let's fetch the lfp data for that probe and that session --\n",
    "            # let's fetch the all the channels falling into V1 domain\n",
    "            self.V1_channel_ids = session.channels[(session.channels.probe_id == probe_id) & \\\n",
    "                          (session.channels.ecephys_structure_acronym.isin(['VISp']))].index.values\n",
    "\n",
    "            # limit LFP to desired times and channels\n",
    "            # N.B. \"get_lfp\" returns a subset of all channels above\n",
    "            self.lfp_slice_V1 = session.get_lfp(probe_id).sel(time=slice(self.t0,\n",
    "                                                                         self.t0+self.duration),\n",
    "                                                              channel=slice(np.min(self.V1_channel_ids), \n",
    "                                                                            np.max(self.V1_channel_ids)))\n",
    "            self.Nchannels_V1 = len(self.lfp_slice_V1.channel) # store number of channels with LFP in V1\n",
    "            self.lfp_sampling_rate = session.probes.lfp_sampling_rate[probe_id] # keeping track of sampling rate\n",
    "            print('data successfully loaded in %.1fs' % (time.time()-tic))\n",
    "              \n",
    "        for key in init:\n",
    "            getattr(self, 'compute_%s' % key)()\n",
    "            \n",
    "\n",
    "    def update_t0_duration(self, t0, duration):\n",
    "        t0 = t0 if (t0 is not None) else self.t0\n",
    "        duration = duration if (duration is not None) else self.duration\n",
    "        return t0, duration\n",
    "    \n",
    "        \n",
    "    def compute_pop_act(self, \n",
    "                        pop_act_bin=5e-3,\n",
    "                        pop_act_smoothing=20e-3):\n",
    "        \"\"\"\n",
    "        we bin spikes to compute population activity\n",
    "        \"\"\"\n",
    "        print(' - computing pop_act from raster [...]') \n",
    "        t_pop_act = self.t0+np.arange(int(self.duration/pop_act_bin)+1)*pop_act_bin\n",
    "        pop_act = np.zeros(len(t_pop_act)-1)\n",
    "\n",
    "        for i, spikes in enumerate(self.V1_RASTER):\n",
    "            pop_act += np.histogram(spikes, bins=t_pop_act)[0]\n",
    "        pop_act /= (len(self.V1_RASTER)*pop_act_bin)\n",
    "\n",
    "        self.t_pop_act = .5*(t_pop_act[1:]+t_pop_act[:-1])\n",
    "        self.pop_act = nsi.gaussian_filter1d(pop_act, \n",
    "                                             int(pop_act_smoothing/pop_act_bin)) # filter from scipy\n",
    "        self.pop_act_sampling_rate = 1./pop_act_bin\n",
    "        print(' - - > done !') \n",
    "        \n",
    "        \n",
    "    def compute_NSI(self, quantity='pLFP',\n",
    "                    low_freqs = np.linspace(2, 5, 5),\n",
    "                    p0_percentile=1.,\n",
    "                    alpha=2.87,\n",
    "                    with_subquantities=True):\n",
    "        \"\"\"\n",
    "        ------------------------------\n",
    "            HERE we use the NSI API\n",
    "        ------------------------------\n",
    "        \"\"\"\n",
    "        print(' - computing NSI for \"%s\" [...]' % quantity) \n",
    "        setattr(self, '%s_0' % quantity, np.percentile(getattr(self, quantity), p0_percentile/100.))\n",
    "        \n",
    "        if with_subquantities:\n",
    "            lfe, sm, NSI = nsi.compute_NSI(getattr(self, quantity),\n",
    "                                           getattr(self, '%s_sampling_rate' % quantity),\n",
    "                                           low_freqs = low_freqs,\n",
    "                                           p0=getattr(self, '%s_0' % quantity),\n",
    "                                           alpha=alpha,\n",
    "                                           with_subquantities=True)\n",
    "            setattr(self, '%s_low_freq_env' % quantity, lfe)\n",
    "            setattr(self, '%s_sliding_mean' % quantity, sm)\n",
    "            setattr(self, '%s_NSI' % quantity, NSI)\n",
    "        \n",
    "        else:\n",
    "            setattr(self, '%s_NSI' % quantity, nsi.compute_NSI(getattr(self, quantity),\n",
    "                                                              getattr(self, '%s_sampling_rate' % quantity),\n",
    "                                                              low_freqs = low_freqs,\n",
    "                                                              p0=getattr(self, '%s_0' % quantity),\n",
    "                                                              alpha=alpha))\n",
    "        print(' - - > done !') \n",
    "        \n",
    "    def validate_NSI(self, quantity='pLFP',\n",
    "                     Tstate=200e-3,\n",
    "                     var_tolerance_threshold=None):\n",
    "        \"\"\"\n",
    "        ------------------------------\n",
    "            HERE we use the NSI API\n",
    "        ------------------------------\n",
    "        \"\"\"\n",
    "        print(' - validating NSI for \"%s\" [...]' % quantity) \n",
    "        \n",
    "        if var_tolerance_threshold is None:\n",
    "            # by default the ~noise level evaluated as the first percentile\n",
    "            var_tolerance_threshold = getattr(self, '%s_0' % quantity)\n",
    " \n",
    "        vNSI = nsi.validate_NSI(getattr(self, 't_%s' % quantity),\n",
    "                                getattr(self, '%s_NSI' % quantity),\n",
    "                                Tstate=Tstate,\n",
    "                                var_tolerance_threshold=var_tolerance_threshold)\n",
    "    \n",
    "        setattr(self, 'i_%s_vNSI' % quantity, vNSI)\n",
    "        setattr(self, 't_%s_vNSI' % quantity, getattr(self, 't_%s' % quantity)[vNSI])\n",
    "        setattr(self, '%s_vNSI' % quantity, getattr(self, '%s_NSI' % quantity)[vNSI])\n",
    "        print(' - - > done !')\n",
    "        \n",
    "    def plot(self, quantity, \n",
    "             t0=None, duration=None,\n",
    "             ax=None,\n",
    "             subsampling=1,\n",
    "             color='k', ms=0,\n",
    "             lw=1):\n",
    "        \"\"\"\n",
    "        quantity as a string (e.g. \"pLFP\" or \"running_speed\")\n",
    "        \"\"\"\n",
    "        \n",
    "        t0, duration = self.update_t0_duration(t0, duration)\n",
    "        \n",
    "        try:\n",
    "            if ax is None:\n",
    "                fig, ax =plt.subplots(1, figsize=(8,3))\n",
    "            else:\n",
    "                fig = None\n",
    "            t = getattr(self, 't_'+quantity.replace('_NSI','').replace('_low_freq_env','').replace('_sliding_mean',''))\n",
    "            signal = getattr(self, quantity)\n",
    "            cond = (t>t0) & (t<(t0+duration))\n",
    "            ax.plot(t[cond][::subsampling], signal[cond][::subsampling], color=color, lw=lw, ms=ms, marker='o')\n",
    "            return fig, ax\n",
    "        except BaseException as be:\n",
    "            print(be)\n",
    "            print('%s not a recognized attribute to plot' % quantity)\n",
    "            return None, None\n",
    "        \n",
    "# a tool very useful to \n",
    "def resample_trace(old_t, old_data, new_t):\n",
    "    func = interp1d(old_t, old_data, kind='nearest', fill_value=\"extrapolate\")\n",
    "    return func(new_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30609d5e-e112-475f-8bd1-fd7b1d4ae5c1",
   "metadata": {},
   "source": [
    "## Channel selection\n",
    "\n",
    "We benefit from many channels (~20 in V1), how to deal with this ? \n",
    "\n",
    "--> simple solution: we pick just one channel, the one that has the highest delta envelope in the pLFP. This sounds like a good guess for a channel with good physiological signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263e6bb-1181-44ba-9570-5f5994b52d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(session_index=0, demo=False, \n",
    "            init=['pop_act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fd6e1-9a8d-4d12-a4a4-c490e06abc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_channel_with_highest_delta(data,\n",
    "                                    pLFP_band=[40,140],\n",
    "                                    delta_band=[3,6],\n",
    "                                    pLFP_subsampling=5):\n",
    "    \n",
    "    channel_mean_delta, channel_id, final_pLFP, final_LFP = 0, None, None, None\n",
    "\n",
    "    for c in range(len(data.lfp_slice_V1.channel.values)):\n",
    "        # first compute pLFP\n",
    "        LFP = 1e3*np.array(data.lfp_slice_V1.sel(channel=data.lfp_slice_V1.channel[c]))\n",
    "        pLFP = 1e3*nsi.compute_freq_envelope(LFP, data.lfp_sampling_rate,\n",
    "                                             np.linspace(pLFP_band[0], pLFP_band[1], 40))\n",
    "        \n",
    "        # then compute low freq envelope of pLFP (subsampled)\n",
    "        lf_env = nsi.compute_freq_envelope(pLFP[::pLFP_subsampling], \n",
    "                                           data.lfp_sampling_rate/pLFP_subsampling,\n",
    "                                           np.linspace(delta_band[0], delta_band[1], 5))\n",
    "        \n",
    "        if np.mean(lf_env)>channel_mean_delta:\n",
    "            channel_mean_delta = np.mean(lf_env)\n",
    "            final_pLFP = pLFP\n",
    "            final_LFP = LFP\n",
    "            channel_id = c\n",
    "            \n",
    "pLFP, LFP, channel_id = find_channel_with_highest_delta(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62eeadc-d78b-460d-99fd-cfc5405f58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, duration = data.t0+120, 5\n",
    "lfp_slice = data.lfp_slice_V1.sel(time=slice(t0,t0+duration))\n",
    "\n",
    "band = [40,140]\n",
    "cmap = plt.cm.copper\n",
    "channel_variance = []\n",
    "\n",
    "fig, AX = plt.subplots(3, figsize=(10,8))\n",
    "for i in range(len(lfp_slice.channel.values)):\n",
    "    AX[0].plot(lfp_slice.time, 1e3*lfp_slice.sel(channel=lfp_slice.channel[i]), \n",
    "                          lw=0.3, color=cmap(1-i/(len(lfp_slice.channel.values)-1)))\n",
    "    pLFP = nsi.compute_freq_envelope(1e3*np.array(lfp_slice.sel(channel=lfp_slice.channel[i])), \n",
    "                                     data.lfp_sampling_rate,\n",
    "                                     np.linspace(band[0], band[1], 40))\n",
    "        \n",
    "    AX[1].plot(lfp_slice.time, 1e3*pLFP, \n",
    "               lw=0.3, color=cmap(1-i/(len(lfp_slice.channel.values)-1)))\n",
    "        \n",
    "data.plot('pop_act', ax=AX[2], t0=t0, duration=duration, color='g')\n",
    "AX[2].set_ylabel('pop. rate (Hz)')\n",
    "AX[0].set_ylabel('LFP (mV)')\n",
    "AX[1].set_ylabel('pLFP (uV)  %sHz' % band)\n",
    "#AX[1].annotate('selected channels: %s' % (np.argsort(channel_variance)[-5:][::-1]),\n",
    "AX[1].annotate('selected channel ID: %i' % channel_id,\n",
    "               (0,1), xycoords='axes fraction', va='top')\n",
    "for i in range(len(lfp_slice.channel.values)):\n",
    "    AX[0].annotate((i+1)*'      '+'                 %i' % i, (0,1), xycoords='axes fraction', va='top',\n",
    "                color=cmap(1-i/(len(lfp_slice.channel.values)-1)))\n",
    "AX[0].annotate('channel ID:\\n(depth-ordered)', (0,1), xycoords='axes fraction', va='top');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd794193-f927-40e1-b9cc-f104724c1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we loop over all sessions to get the reduced data with those properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f06ae4-5a56-495d-acef-69b71ca8321d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def save_reduced_data(data, session_index, LFP, pLFP, channel_id):\n",
    "    new_data = {'t0':data.t0, 'duration':data.duration,\n",
    "                'pop_act':data.pop_act, 'pop_act_sampling_rate':data.pop_act_sampling_rate,\n",
    "                'selected_channel_id':channel_id,\n",
    "                'LFP':LFP, 'LFP_sampling_rate':data.lfp_sampling_rate,\n",
    "                'pLFP':pLFP, 'pLFP_sampling_rate':data.lfp_sampling_rate,\n",
    "                't_running_speed':data.t_running_speed, 'running_speed':data.running_speed,\n",
    "               }\n",
    "    np.save('demo/reduced_data/Allen_FC_session%i.npy' % (session_index+1), new_data)\n",
    "\n",
    "    \n",
    "for session_index in range(len(sessions)):\n",
    "    \n",
    "    data = Data(session_index=session_index, demo=False, \n",
    "                init=['pop_act'])\n",
    "    pLFP, LFP, channel_id = find_channel_with_highest_delta(data)\n",
    "    save_reduced_data(data, session_index, LFP, pLFP, channel_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = Data0(0, t0=110*60, duration=20*60, init=['raster', 'pop_act'])\n",
    "\n",
    "\n",
    "fig, ax = data.plot('running_speed')\n",
    "ax.set_ylabel('speed (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30a82c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "def plot_sample_data(data,\n",
    "                     time_points=[100, 1000],\n",
    "                     duration=2., \n",
    "                     pop_act_bin=5e-3):\n",
    "\n",
    "    nplots = 6 if hasattr(data, 'V1_RASTER') else 5\n",
    "    fig, AX_full = plt.subplots(nplots,len(time_points), figsize=(3*len(time_points), 7))\n",
    "    if len(time_points)==1:\n",
    "        AX_full = [AX_full]\n",
    "        \n",
    "    YLIMS = [[np.inf, -np.inf] for i in range(len(AX_full))]\n",
    "    for t0, AX in zip(time_points, AX_full.T):\n",
    "        \n",
    "        # raster plot\n",
    "        if hasattr(data, 'V1_RASTER'):\n",
    "            for i, spikes in enumerate(data.V1_RASTER):\n",
    "                cond = (spikes>t0) & (spikes<(t0+duration))\n",
    "                AX[0].plot(spikes[cond], i+0*spikes[cond], 'ko', ms=1.5)\n",
    "\n",
    "        # pop act. plot\n",
    "        data.plot('pop_act', t0=t0, duration=duration, ax=AX[nplots-5])\n",
    "        \n",
    "        # LFP plot\n",
    "        data.plot('LFP', t0=t0, duration=duration, ax=AX[nplots-4])\n",
    "        \n",
    "        # pLFP plot\n",
    "        data.plot('pLFP', t0=t0, duration=duration, ax=AX[nplots-3], color=plt.cm.tab10(5))\n",
    "\n",
    "        # NSI plot\n",
    "        data.plot('pLFP_NSI', t0=t0, duration=duration, ax=AX[nplots-2], color=plt.cm.tab10(5))\n",
    "\n",
    "        # speed plot\n",
    "        data.plot('running_speed', t0=t0, duration=duration, ax=AX[nplots-1])\n",
    "\n",
    "        # labelling axes and setting the same limes\n",
    "        for j, label, ax in zip(range(len(AX)), \n",
    "                                ['units', 'rate (Hz)', 'LFP (mV)', 'pLFP (uV)', 'NSI (uV)', 'run. speed\\n(cm/s)'],\n",
    "                                AX):\n",
    "            if ax in AX_full.T[0]:\n",
    "                ax.set_ylabel(label)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_xlim([t0,t0+duration])\n",
    "            YLIMS[j] = [np.min([AX[j].get_ylim()[0], YLIMS[j][0]]),\n",
    "                        np.max([AX[j].get_ylim()[1], YLIMS[j][1]])]\n",
    "        AX[0].set_title('$t_0$=%.1fs' % (t0), size='small')\n",
    "        \n",
    "    for AX in AX_full.T:\n",
    "        for j in range(len(AX)):\n",
    "            try:\n",
    "                AX[j].set_ylim(YLIMS[j])\n",
    "            except BaseException:\n",
    "                pass\n",
    "    for t0, AX in zip(time_points, AX_full.T):\n",
    "        AX[1].plot([t0,t0+0.2], .9*YLIMS[1][1]*np.ones(2), 'k-', lw=1)\n",
    "        AX[1].annotate('200ms', (t0, .92*YLIMS[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4014b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(session_index=0, demo=False, reduced=True)\n",
    "plot_sample_data(data, \n",
    "                 time_points=[6605.5, 6602.3, 7245]) # session 1 -- functional_connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca56b8f4-a135-4cd2-b2b6-a6cc4cb759a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d \n",
    "\n",
    "def get_accuracy(data,\n",
    "                 rate_tolerance=2,\n",
    "                 with_fig=True):\n",
    "    \n",
    "    pop_act_NSI_resampled = resample_trace(data.t_pop_act, \n",
    "                                           data.pop_act_NSI, data.t_pLFP)\n",
    "    \n",
    "\n",
    "    x, y = data.pLFP_NSI[data.i_pLFP_vNSI], pop_act_NSI_resampled[data.i_pLFP_vNSI]\n",
    "    cond = ((x>0) & (y>0)) | ((x<0) & (y<0))\n",
    "\n",
    "    lin = np.polyfit(x[cond], y[cond], 1)\n",
    "    \n",
    "    accuracy_cond = np.abs(y-np.polyval(lin, x))<rate_tolerance\n",
    "    \n",
    "    accuracy = 100*np.sum(accuracy_cond)/len(y)    \n",
    "    if with_fig:\n",
    "        fig, ax = plt.subplots(1, figsize=(2,2))\n",
    "        ax.set_title('accuracy=%.1f%%'%accuracy)\n",
    "        x = np.linspace(x.min(), x.max())\n",
    "        ax.plot(data.pLFP_NSI[data.i_pLFP_vNSI], \n",
    "                 pop_act_NSI_resampled[data.i_pLFP_vNSI], 'o', lw=1, ms=1)\n",
    "        ax.fill_between(x, np.polyval(lin, x)-rate_tolerance, np.polyval(lin, x)+rate_tolerance, color='g', alpha=.3)\n",
    "        ax.plot(x, np.polyval(lin, x), 'k-')\n",
    "        ax.set_ylabel('NSI$_{\\,rate}$ (Hz)')\n",
    "        ax.set_xlabel('NSI$_{\\,pLFP}$ ($\\mu$V)')\n",
    "        return fig, ax, accuracy\n",
    "    else:\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce12281-de6f-4246-bac8-38458bc4b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(session_index=5, demo=False, reduced=True)\n",
    "            \n",
    "data.compute_NSI(quantity='pLFP',\n",
    "                 alpha=2.8,\n",
    "                 p0_percentile=1)\n",
    "data.compute_NSI(quantity='pop_act',\n",
    "                 alpha=2,\n",
    "                 p0_percentile=0)\n",
    "data.validate_NSI(quantity='pLFP')\n",
    "\n",
    "get_accuracy(data)\n",
    "#ax.set_ylabel('speed (cm/s)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
