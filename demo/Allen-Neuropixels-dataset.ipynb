{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9476069",
   "metadata": {},
   "source": [
    "# Network State Index -- API demo\n",
    "\n",
    "see: https://github.com/yzerlaut/Network_State_Index\n",
    "\n",
    "We demonstrate the use of the API on the following publicly available dataset:\n",
    "\n",
    "## the \"Visual Coding â€“ Neuropixels\" dataset from the Allen Observatory\n",
    "\n",
    "All details about this dataset and instructions for analysis are available at:\n",
    "\n",
    "https://allensdk.readthedocs.io/en/latest/visual_coding_neuropixels.html\n",
    "\n",
    "## Dataset download\n",
    "\n",
    "I adapted the download instructions and made a [custom script](https://github.com/yzerlaut/Network_State_Index/blob/main/demo/download_Allen_Visual-Coding_dataset.py) to download exclusively the part of the dataset of interest here (i.e. V1 probes, ~20GB of LFP data).\n",
    "You can run the script as:\n",
    "```\n",
    "python demo/download_Allen_Visual-Coding_dataset.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392f4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some general python / scientific-python modules\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "#plt.style.use('seaborn')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# the Network State Index API, see: https://github.com/yzerlaut/Network_State_Index\n",
    "# install it with: \"pip install git+https://github.com/yzerlaut/Network_State_Index\"\n",
    "import nsi\n",
    "\n",
    "\n",
    "V1_Allen_params = {'pLFP_band':[40,140], # Hz -- paper's values \n",
    "                   'delta_band':[4,8], # Hz -- the delta peaks at 6hz, not 3hz..\n",
    "                   'Tsmoothing':30e-3,  # s -- slightly lower smoothing because the 42ms smoothing of S1 smooth out a bit too much the delta in V1\n",
    "                   'alpha_LFP':3., # evaluated below\n",
    "                   'alpha_rate':2} # the rate is a virtually noiseless signal (it defines Network States), so no need to be >2 (see paper's derivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68248afc",
   "metadata": {},
   "source": [
    "### We restrict the analysis to:\n",
    "\n",
    "- wild type / wild type strain\n",
    "- male\n",
    "- sessions with probes in V1 (\"VISp\")\n",
    "- \"functional_connectivity\" dataset because it has 30min of spontaneous activity (we pick 20min in the center of that period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f38353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "--> Number of sessions with the desired characteristics: 11\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download and load the data with the \"allensdk\" API\n",
    "# get the \"allensdk\" api with: \"pip install allensdk\"\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "\n",
    "# now let's define a cache repository for the data: by default ~/Downloads/ecephys_cache_dir\n",
    "# insure that you have a \"Downloads\" repository in your home directory (/!\\ non-english systems) or update below\n",
    "data_directory = os.path.join(os.path.expanduser('~'), 'Downloads', 'ecephys_cache_dir')\n",
    "manifest_path = os.path.join(data_directory, \"manifest.json\")\n",
    "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
    "all_sessions = cache.get_session_table() # get all sessions\n",
    "\n",
    "# let's filter the sessions according to the above criteria\n",
    "sessions = all_sessions[(all_sessions.sex == 'M') & \\\n",
    "                        (all_sessions.full_genotype.str.find('wt/wt') > -1) & \\\n",
    "                        #(all_sessions.session_type == 'brain_observatory_1.1') & \\\n",
    "                        (all_sessions.session_type == 'functional_connectivity') & \\\n",
    "                        (['VISp' in acronyms for acronyms in all_sessions.ecephys_structure_acronyms])]\n",
    "print(30*'--'+'\\n--> Number of sessions with the desired characteristics: ' + str(len(sessions))+'\\n'+30*'--')\n",
    "# sessions.head() # uncomment to see how they look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cadf5",
   "metadata": {},
   "source": [
    "## Loading, formatting and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d955ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "   \n",
    "class Data:\n",
    "    \"\"\"\n",
    "    an object to load, format and process the data\n",
    "    we use the Allen SDK to fetch the V1 channels\n",
    "    \n",
    "    we format things so that data are accessible as: \"data.QUANTITY\" with time sampling data.t_QUANTITY (e.g. \"data.pLFP\", \"data.t_pLFP\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 session_index=0,\n",
    "                 reduced=False,\n",
    "                 demo=False, demo_filename='allen_demo_sample.npy',\n",
    "                 t0=115*60, \n",
    "                 duration=20*60, # 20 min by default\n",
    "                 #init=['pop_act', 'pLFP', 'NSI'], # for a full init\n",
    "                 init = []):\n",
    "        \"\"\"\n",
    "        loading data:\n",
    "        - the Allen NWB files are loaded by default, select the session according to the index of the \"sessions\" above\n",
    "        - for initial troubleshooting, there is a \"demo\" option that loads a lightweight data sample provided in the repo\n",
    "        - for faster analysis alter (to loop over the data), we can load the \"reduced\" version of the data of interest (generated below)\n",
    "\n",
    "\n",
    "        can load a subset using the \"t0\" and \"duration\" args\n",
    "        \"\"\"\n",
    "        \n",
    "        if demo:\n",
    "            # -------- using the stored demo data if \"demo\" mode ----- #\n",
    "            DEMO = np.load(demo_filename, allow_pickle=True).item()\n",
    "            for key in DEMO:\n",
    "                setattr(self, key, DEMO[key]) # sets LFP\n",
    "            self.t_LFP = np.arange(len(self.LFP))/self.lfp_sampling_rate+self.t0\n",
    "        elif reduced:\n",
    "            rdata = np.load('reduced_data/Allen_FC_session%i.npy' % (session_index+1), allow_pickle=True).item()\n",
    "            for key in rdata:\n",
    "                setattr(self, key, rdata[key]) # sets LFP, pLFP, pop_act, running_speed\n",
    "            for key in ['LFP', 'pLFP', 'pop_act']:\n",
    "                setattr(self, 't_%s'%key, np.arange(len(getattr(self,key)))/getattr(self,'%s_sampling_rate'%key)+self.t0)\n",
    "        else:\n",
    "            # -------------------------------------------------------- # \n",
    "            # -- using the Allen SDK to retrieve and cache the data -- #\n",
    "            # -------------------------------------------------------- # \n",
    "\n",
    "            print('loading session #%i [...]' % (1+session_index))\n",
    "            tic = time.time()\n",
    "            # we load a single session\n",
    "            session = cache.get_session_data(sessions.index.values[session_index])\n",
    "\n",
    "            # use the running timestamps to set start and duration in the data object\n",
    "            self.t0 = np.max([t0, session.running_speed.start_time.values[0]])\n",
    "            self.duration = np.min([duration, session.running_speed.end_time.values[-1]-self.t0])\n",
    "\n",
    "            # let's fetch the running speed\n",
    "            cond = (session.running_speed.end_time.values>self.t0) &\\\n",
    "                (session.running_speed.start_time.values<(self.t0+self.duration))\n",
    "            self.t_running_speed = .5*(session.running_speed.start_time.values[cond]+\\\n",
    "                                       session.running_speed.end_time.values[cond])\n",
    "            self.running_speed = session.running_speed.velocity[cond]\n",
    "\n",
    "            # let's fetch the isolated single units in V1\n",
    "            V1_units = session.units[session.units.ecephys_structure_acronym == 'VISp'] # V1==VISp\n",
    "            self.V1_RASTER = []\n",
    "            for i in V1_units.index:\n",
    "                cond = (session.spike_times[i]>=self.t0) & (session.spike_times[i]<(self.t0+self.duration))\n",
    "                self.V1_RASTER.append(session.spike_times[i][cond])\n",
    "\n",
    "            # let's fetch the V1 probe --> always on \"probeC\"\n",
    "            probe_id = session.probes[session.probes.description == 'probeC'].index.values[0]\n",
    "\n",
    "            # -- let's fetch the lfp data for that probe and that session --\n",
    "            # let's fetch the all the channels falling into V1 domain\n",
    "            self.V1_channel_ids = session.channels[(session.channels.probe_id == probe_id) & \\\n",
    "                          (session.channels.ecephys_structure_acronym.isin(['VISp']))].index.values\n",
    "\n",
    "            # limit LFP to desired times and channels\n",
    "            # N.B. \"get_lfp\" returns a subset of all channels above\n",
    "            self.lfp_slice_V1 = session.get_lfp(probe_id).sel(time=slice(self.t0,\n",
    "                                                                         self.t0+self.duration),\n",
    "                                                              channel=slice(np.min(self.V1_channel_ids), \n",
    "                                                                            np.max(self.V1_channel_ids)))\n",
    "            self.Nchannels_V1 = len(self.lfp_slice_V1.channel) # store number of channels with LFP in V1\n",
    "            self.lfp_sampling_rate = session.probes.lfp_sampling_rate[probe_id] # keeping track of sampling rate\n",
    "            print('data successfully loaded in %.1fs' % (time.time()-tic))\n",
    "              \n",
    "        for key in init:\n",
    "            getattr(self, 'compute_%s' % key)()\n",
    "            \n",
    "\n",
    "    def update_t0_duration(self, t0, duration):\n",
    "        t0 = t0 if (t0 is not None) else self.t0\n",
    "        duration = duration if (duration is not None) else self.duration\n",
    "        return t0, duration\n",
    "    \n",
    "        \n",
    "    def compute_pop_act(self, \n",
    "                        pop_act_bin=5e-3,\n",
    "                        pop_act_smoothing=V1_Allen_params['Tsmoothing']):\n",
    "        \"\"\"\n",
    "        we bin spikes to compute population activity\n",
    "        \"\"\"\n",
    "        print(' - computing pop_act from raster [...]') \n",
    "        t_pop_act = self.t0+np.arange(int(self.duration/pop_act_bin)+1)*pop_act_bin\n",
    "        pop_act = np.zeros(len(t_pop_act)-1)\n",
    "\n",
    "        for i, spikes in enumerate(self.V1_RASTER):\n",
    "            pop_act += np.histogram(spikes, bins=t_pop_act)[0]\n",
    "        pop_act /= (len(self.V1_RASTER)*pop_act_bin)\n",
    "\n",
    "        self.t_pop_act = .5*(t_pop_act[1:]+t_pop_act[:-1])\n",
    "        self.pop_act = nsi.gaussian_filter1d(pop_act, \n",
    "                                             int(pop_act_smoothing/pop_act_bin)) # filter from scipy\n",
    "        self.pop_act_sampling_rate = 1./pop_act_bin\n",
    "        print(' - - > done !') \n",
    "        \n",
    "        \n",
    "    def compute_NSI(self, quantity='pLFP',\n",
    "                    low_freqs = np.linspace(*V1_Allen_params['delta_band'], 5),\n",
    "                    p0_percentile=1.,\n",
    "                    alpha=2.87,\n",
    "                    T_sliding_mean=500e-3,\n",
    "                    with_subquantities=True,\n",
    "                    verbose=True):\n",
    "        \"\"\"\n",
    "        ------------------------------\n",
    "            HERE we use the NSI API\n",
    "        ------------------------------\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(' - computing NSI for \"%s\" [...]' % quantity) \n",
    "        setattr(self, '%s_0' % quantity, np.percentile(getattr(self, quantity), p0_percentile/100.))\n",
    "        \n",
    "        lfe, sm, NSI = nsi.compute_NSI(getattr(self, quantity),\n",
    "                                       getattr(self, '%s_sampling_rate' % quantity),\n",
    "                                       low_freqs = low_freqs,\n",
    "                                       p0=getattr(self, '%s_0' % quantity),\n",
    "                                       alpha=alpha,\n",
    "                                       T_sliding_mean=T_sliding_mean, \n",
    "                                       with_subquantities=True) # we fetch also the NSI subquantities (low-freq env and sliding mean), set below !\n",
    "        setattr(self, '%s_low_freq_env' % quantity, lfe)\n",
    "        setattr(self, '%s_sliding_mean' % quantity, sm)\n",
    "        setattr(self, '%s_NSI' % quantity, NSI)\n",
    "        if verbose:\n",
    "            print(' - - > done !') \n",
    "        \n",
    "    def validate_NSI(self, quantity='pLFP',\n",
    "                     Tstate=200e-3,\n",
    "                     var_tolerance_threshold=None,\n",
    "                     verbose=True):\n",
    "        \"\"\"\n",
    "        ------------------------------\n",
    "            HERE we use the NSI API\n",
    "        ------------------------------\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(' - validating NSI for \"%s\" [...]' % quantity) \n",
    "        \n",
    "        if var_tolerance_threshold is None:\n",
    "            # by default the ~noise level evaluated as the first percentile\n",
    "            var_tolerance_threshold = getattr(self, '%s_0' % quantity)\n",
    " \n",
    "        vNSI = nsi.validate_NSI(getattr(self, 't_%s' % quantity),\n",
    "                                getattr(self, '%s_NSI' % quantity),\n",
    "                                Tstate=Tstate,\n",
    "                                var_tolerance_threshold=var_tolerance_threshold)\n",
    "    \n",
    "        setattr(self, 'i_%s_vNSI' % quantity, vNSI)\n",
    "        setattr(self, 't_%s_vNSI' % quantity, getattr(self, 't_%s' % quantity)[vNSI])\n",
    "        setattr(self, '%s_vNSI' % quantity, getattr(self, '%s_NSI' % quantity)[vNSI])\n",
    "        if verbose:\n",
    "            print(' - - > done !')\n",
    "        \n",
    "    def plot(self, quantity, \n",
    "             t0=None, duration=None,\n",
    "             ax=None, label='',\n",
    "             subsampling=1,\n",
    "             color='k', ms=0, lw=1, alpha=1):\n",
    "        \"\"\"\n",
    "        quantity as a string (e.g. \"pLFP\" or \"running_speed\")\n",
    "        \"\"\"\n",
    "        \n",
    "        t0, duration = self.update_t0_duration(t0, duration)\n",
    "        \n",
    "        try:\n",
    "            if ax is None:\n",
    "                fig, ax =plt.subplots(1, figsize=(8,3))\n",
    "            else:\n",
    "                fig = None\n",
    "            t = getattr(self, 't_'+quantity.replace('_NSI','').replace('_low_freq_env','').replace('_sliding_mean',''))\n",
    "            signal = getattr(self, quantity)\n",
    "            cond = (t>t0) & (t<(t0+duration))\n",
    "            ax.plot(t[cond][::subsampling], signal[cond][::subsampling], color=color, lw=lw, ms=ms, marker='o', alpha=alpha)\n",
    "            ax.set_ylabel(label)\n",
    "            return fig, ax\n",
    "        except BaseException as be:\n",
    "            print(be)\n",
    "            print('%s not a recognized attribute to plot' % quantity)\n",
    "            return None, None\n",
    "        \n",
    "from scipy.interpolate import interp1d\n",
    "# a tool very useful to \n",
    "def resample_trace(old_t, old_data, new_t):\n",
    "    func = interp1d(old_t, old_data, kind='nearest', fill_value=\"extrapolate\")\n",
    "    return func(new_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30609d5e-e112-475f-8bd1-fd7b1d4ae5c1",
   "metadata": {},
   "source": [
    "## Channel selection in LFP recordings with Neuropixel probes\n",
    "\n",
    "We benefit from many channels in the area of interest (~20 in V1). How to deal with this ? \n",
    "\n",
    "--> simple solution: we pick just one channel, the one that has the highest delta envelope in the pLFP. This sounds like a good guess for a channel with good physiological signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999fd6e1-9a8d-4d12-a4a4-c490e06abc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_channel_with_highest_delta(data,\n",
    "                                    t0=None, duration=None,\n",
    "                                    pLFP_band=V1_Allen_params['pLFP_band'],\n",
    "                                    delta_band=V1_Allen_params['delta_band'],\n",
    "                                    pLFP_subsampling=5,\n",
    "                                    return_all=False):\n",
    "    \"\"\"\n",
    "    A function to pick the \n",
    "    \"\"\"\n",
    "    channel_mean_delta, channel_id, final_pLFP, final_LFP = 0, None, None, None\n",
    "\n",
    "    \n",
    "    ALL = None\n",
    "    if return_all:\n",
    "        ALL = {'LFP':[], 'pLFP':[], 't':data.lfp_slice_V1.time}\n",
    "\n",
    "    for c in range(len(data.lfp_slice_V1.channel.values)):\n",
    "        # first get the LFP\n",
    "        LFP = 1e3*np.array(data.lfp_slice_V1.sel(channel=data.lfp_slice_V1.channel[c]))\n",
    "        if return_all:\n",
    "            ALL['LFP'].append(LFP)\n",
    "        # then compute the pLFP            === USING the *nsi* API ===\n",
    "        _, pLFP = nsi.compute_pLFP(LFP, data.lfp_sampling_rate,\n",
    "                                   smoothing=V1_Allen_params['Tsmoothing'])\n",
    "        if return_all:\n",
    "            ALL['pLFP'].append(1e3*pLFP)\n",
    "        # compute low freq envelope of pLFP (subsampled on need of full sampling)\n",
    "        lf_env = nsi.compute_freq_envelope(1e3*pLFP[::pLFP_subsampling], \n",
    "                                           data.lfp_sampling_rate/pLFP_subsampling,\n",
    "                                           np.linspace(delta_band[0], delta_band[1], 5))\n",
    "        \n",
    "        if np.mean(lf_env)>channel_mean_delta:\n",
    "            channel_mean_delta = np.mean(lf_env)\n",
    "            final_LFP = LFP\n",
    "            final_pLFP = 1e3*pLFP\n",
    "            channel_id = c\n",
    "    return final_LFP, final_pLFP, channel_id, ALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cade97-47e2-4811-b2ff-88bec1c93be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading session #1 [...]\n",
      "data successfully loaded in 34.8s\n",
      " - computing pop_act from raster [...]\n",
      " - - > done !\n"
     ]
    }
   ],
   "source": [
    "data = Data(session_index=0, \n",
    "            init=['pop_act'])\n",
    "\n",
    "final_LFP, final_pLFP, channel_id, ALL = find_channel_with_highest_delta(data,\n",
    "                                                                         return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62eeadc-d78b-460d-99fd-cfc5405f58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, duration = data.t0+120, 5 # showing a sample with some nice sinal variations\n",
    "fig, AX = plt.subplots(3, figsize=(9,6))\n",
    "cmap = plt.cm.copper\n",
    "\n",
    "cond = (ALL['t']>t0) & (ALL['t']<(t0+duration))\n",
    "AX[1].plot(ALL['t'][cond], final_pLFP[cond], lw=3, color='darkgrey')\n",
    "for i in range(len(ALL['LFP'])):\n",
    "    AX[0].plot(ALL['t'][cond], ALL['LFP'][i][cond], lw=0.3, color=cmap(1-i/(data.Nchannels_V1-1)))\n",
    "    AX[1].plot(ALL['t'][cond], ALL['pLFP'][i][cond], lw=0.3, color=cmap(1-i/(data.Nchannels_V1-1)))\n",
    "        \n",
    "data.plot('pop_act', ax=AX[2], t0=t0, duration=duration)\n",
    "AX[2].set_ylabel('rate (Hz)  ')\n",
    "AX[2].set_xlabel('time (s)  ')\n",
    "AX[0].set_ylabel('LFP (mV)')\n",
    "AX[1].set_ylabel('pLFP (uV)')\n",
    "AX[1].annotate('selected channel ID: %i' % channel_id,\n",
    "               (1,.9), xycoords='axes fraction', va='top', ha='right',color='darkgrey', weight='bold')\n",
    "for i in range(data.Nchannels_V1):\n",
    "    AX[0].annotate((i+1)*'      '+'                 %i' % i, (0,1), xycoords='axes fraction', va='top',\n",
    "                color=cmap(1-i/(data.Nchannels_V1-1)))\n",
    "AX[0].annotate('channel ID:\\n(depth-ordered)', (0,1), xycoords='axes fraction', va='top');\n",
    "plt.tight_layout()\n",
    "fig.savefig('../doc/Neuropixels-channel-selection.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f367ff9-bd52-4c6d-9303-390003c2af0c",
   "metadata": {},
   "source": [
    "#### Now we loop over all sessions to get the reduced data with those properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f06ae4-5a56-495d-acef-69b71ca8321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 10 minutes on a good desktop computer\n",
    "\n",
    "def save_reduced_data(data, session_index, LFP, pLFP, channel_id):\n",
    "    new_data = {'t0':data.t0, 'duration':data.duration,\n",
    "                'V1_RASTER':data.V1_RASTER,\n",
    "                'pop_act':data.pop_act, 'pop_act_sampling_rate':data.pop_act_sampling_rate,\n",
    "                'selected_channel_id':channel_id,\n",
    "                'LFP':LFP, 'LFP_sampling_rate':data.lfp_sampling_rate,\n",
    "                'pLFP':pLFP, 'pLFP_sampling_rate':data.lfp_sampling_rate,\n",
    "                't_running_speed':data.t_running_speed, 'running_speed':data.running_speed,\n",
    "               }\n",
    "    np.save('reduced_data/Allen_FC_session%i.npy' % (session_index+1), new_data)\n",
    "\n",
    "    \n",
    "for session_index in range(len(sessions)):\n",
    "    \n",
    "    data = Data(session_index=session_index, reduced=False, demo=False,\n",
    "                init=['pop_act'])\n",
    "    print('looking for the best recording channel [...]')\n",
    "    LFP, pLFP, channel_id, _ = find_channel_with_highest_delta(data)\n",
    "    save_reduced_data(data, session_index, LFP, pLFP, channel_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e137f85-da12-4bba-b725-b8f6a43e7239",
   "metadata": {},
   "source": [
    "## Characterizing the delta oscillation in V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f840dde7-231c-4086-ba87-7dd5ed36d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.linspace(1, 10, 40)\n",
    "\n",
    "envelope = np.zeros((len(freqs), len(sessions)))\n",
    "\n",
    "for i in range(len(sessions)):\n",
    "    data = Data(session_index=i, reduced=True)\n",
    "    # wavelet transform using \"NSI\"\n",
    "    envelope[:,i] = 1e3*np.abs(nsi.my_cwt(data.LFP, freqs, \n",
    "                                      1./data.LFP_sampling_rate)).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7f19f0d-5694-4e5e-a8a4-a191036a8b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in V1, delta peak at: 5.7 +/- 0.6 Hz\n"
     ]
    }
   ],
   "source": [
    "max_env_levels = [freqs[np.argmax(envelope[:,i])] for i in range(len(sessions))]\n",
    "print('in V1, delta peak at: %.1f +/- %.1f Hz' % (np.mean(max_env_levels), np.std(max_env_levels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7144478a-4bcd-421c-b6cb-e7ce63bfbb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare to S1, loading a reduced dataset per cell\n",
    "\n",
    "if os.path.isfile('reduced_data/reduced_data_cell_1.npz'):\n",
    "    # if you have the S1 data, let's do the same on S1\n",
    "    envelope_S1 = np.zeros((len(freqs), 14))\n",
    "    for i in np.arange(1, 15):\n",
    "        data = dict(np.load('reduced_data/reduced_data_cell_'+str(i)+'.npz'))\n",
    "        envelope_S1[:,i-1] = 1e3*np.abs(nsi.my_cwt(data['sbsmpl_Extra'], freqs, \n",
    "                                             data['sbsmpl_dt'])).mean(axis=1)\n",
    "else:\n",
    "    envelope_S1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3b095-07e4-4be6-8bd9-35e556d6277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(2.3,2.3))\n",
    "\n",
    "if envelope_S1 is not None:\n",
    "    ax.plot(freqs, envelope_S1.mean(axis=1), color='darkgreen', lw=1)\n",
    "    ax.fill_between(freqs, \n",
    "                     envelope_S1.mean(axis=1)-envelope_S1.std(axis=1),\n",
    "                     envelope_S1.mean(axis=1)+envelope_S1.std(axis=1), color='darkgreen', alpha=0.2, lw=0)\n",
    "    ax.annotate('S1           \\n', (1,0.05), color='darkgreen', xycoords='axes fraction', ha='right')\n",
    "    \n",
    "ax.plot(freqs, envelope.mean(axis=1), color='darkblue')\n",
    "ax.fill_between(freqs, \n",
    "                 envelope.mean(axis=1)-envelope.std(axis=1),\n",
    "                 envelope.mean(axis=1)+envelope.std(axis=1), color='darkblue', alpha=0.3, lw=0)\n",
    "ax.annotate('V1 - Allen', (1,0.05), color='darkblue', xycoords='axes fraction', ha='right')\n",
    "ax.set_xticks([2,4,6,8,10])\n",
    "ax.set_xlabel('freq. (Hz)')\n",
    "ax.set_ylabel('LFP env. ($\\mu$V)')\n",
    "plt.tight_layout()\n",
    "fig.savefig('../doc/delta-oscill-comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc81535-32b1-40a8-bbaf-52f286ad1d4c",
   "metadata": {},
   "source": [
    "## Find the optimal $\\alpha$ parameter\n",
    "\n",
    "this loop over session and $\\alpha$ values is a bit long to run, ~15min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "370e6af1-ea6d-408d-ab17-61e91d5ca585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yann.zerlaut/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/yann.zerlaut/miniconda3/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "alpha_values = np.linspace(1.5, 7, 12)\n",
    "\n",
    "output = np.zeros((len(sessions), len(alpha_values)))\n",
    "for i in range(len(sessions)):\n",
    "    #print('session', i)\n",
    "    data = Data(session_index=i, reduced=True)\n",
    "    data.compute_NSI(quantity='pop_act', alpha=2, verbose=False)\n",
    "    for a, alpha in enumerate(alpha_values):\n",
    "        data.compute_NSI(quantity='pLFP',\n",
    "                         alpha=alpha,\n",
    "                         p0_percentile=1,\n",
    "                         verbose=False)\n",
    "        data.validate_NSI(quantity='pLFP', verbose=False)\n",
    "        pop_act_low_freq_env = resample_trace(data.t_pop_act, data.pop_act_low_freq_env, data.t_pLFP)\n",
    "        delta_cond_validated = data.i_pLFP_vNSI & (data.pLFP_NSI<0)\n",
    "        output[i, a] = np.mean(pop_act_low_freq_env[delta_cond_validated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58acb71-9ccf-4a59-928e-27c99700ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting and fitting the decay\n",
    "fig, ax = plt.subplots(1, figsize=(2.3,2.3))\n",
    "\n",
    "# fitting an exponential\n",
    "from scipy.optimize import curve_fit\n",
    "def exp(t, decay,\n",
    "        t0=alpha_values[0],\n",
    "        x0=np.nanmean(output, axis=0)[-1],\n",
    "        x1=np.nanmean(output, axis=0)[0]):\n",
    "    return x0+(x1-x0)*np.exp(-(t-t0)/decay)\n",
    "fit = curve_fit(exp, alpha_values, np.nanmean(output, axis=0), p0=10)\n",
    "    \n",
    "# plotting\n",
    "ax.plot(alpha_values, np.nanmean(output, axis=0), color='darkblue')\n",
    "ax.plot(alpha_values, exp(alpha_values, fit[0]), 'r:')\n",
    "ax.annotate('$\\\\alpha_{top}$=%.2f' % (alpha_values[0]+fit[0]), (1,.7), ha='right', va='top', xycoords='axes fraction', color='r')\n",
    "ax.fill_between(alpha_values, \n",
    "                 np.nanmean(output, axis=0)-np.nanstd(output, axis=0),\n",
    "                 np.nanmean(output, axis=0)+np.nanstd(output, axis=0),color='darkblue', alpha=0.3, lw=0)\n",
    "ax.annotate('n=11 sessions\\nV1 - Allen', (1,1), ha='right', va='top', xycoords='axes fraction', color='darkblue')\n",
    "ax.set_xlabel('alpha (unitless)')\n",
    "ax.set_ylabel('rate $\\delta$ env. (Hz)')\n",
    "plt.tight_layout()\n",
    "fig.savefig('../doc/alpha-delta-dep.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dcf31-6d0d-45be-8e14-6a2821cfae46",
   "metadata": {},
   "source": [
    "## Characterizing Network State Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "656dfa83-97f9-4996-8889-a9aa2255314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSI_bins = np.linspace(-50, 80, 50)\n",
    "NSI_hist = np.zeros((len(sessions), len(NSI_bins)-1))\n",
    "for i in range(len(sessions)):\n",
    "    data = Data(session_index=i, reduced=True)\n",
    "    data.compute_NSI(quantity='pLFP',\n",
    "                     alpha=3.11,\n",
    "                     p0_percentile=1,\n",
    "                    verbose=False)\n",
    "    data.validate_NSI(quantity='pLFP', verbose=False)\n",
    "    NSI_hist[i,:] = np.histogram(data.pLFP_vNSI, bins=NSI_bins, density=True)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e1679-ca73-4cdf-bfcc-938aeb58ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(2.3,2.3))\n",
    "x = .5*(NSI_bins[1:]+NSI_bins[:-1])\n",
    "for i in range(NSI_hist.shape[0]):\n",
    "    ax.plot(x, NSI_hist[i,:], '-', lw=0.2, color='grey')\n",
    "ax.annotate('run. disk\\nV1 - Allen', (0,1), xycoords='axes fraction', color='darkblue', va='top')\n",
    "ax.annotate('n=11 sessions', (1,1), xycoords='axes fraction', color='darkblue', va='top', ha='right', rotation=90, size='small')\n",
    "ax.plot(x, NSI_hist.mean(axis=0), '-', color='darkblue')\n",
    "ax.fill_between(x, NSI_hist.mean(axis=0)-NSI_hist.std(axis=0), NSI_hist.mean(axis=0)+NSI_hist.std(axis=0), color='darkblue', alpha=.3)\n",
    "ax.set_ylabel('norm. count')\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('NSI$_{pLFP}$ ($\\mu$V)')\n",
    "ax.set_xticks([-30,0,30,60])\n",
    "plt.tight_layout()\n",
    "fig.savefig('../doc/V1-NSI-hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d08a5-189f-43c8-ab0a-512c46796bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minc=1e-4 # min count\n",
    "fig, ax = plt.subplots(1, figsize=(2.3,2.3))\n",
    "x = .5*(NSI_bins[1:]+NSI_bins[:-1])\n",
    "for i in range(NSI_hist.shape[0]):\n",
    "    ax.plot(x, NSI_hist[i,:]+minc, '-', lw=0.2, color='grey')\n",
    "ax.annotate('run. disk\\nV1 - Allen', (0,1), xycoords='axes fraction', color='darkblue', va='top')\n",
    "ax.annotate('n=11 sessions', (1,1), xycoords='axes fraction', color='darkblue', va='top', ha='right', rotation=90, size='small')\n",
    "mean = NSI_hist.mean(axis=0)\n",
    "mean[mean<minc] = minc\n",
    "ax.plot(x, mean, '-', color='darkblue')\n",
    "lb, hb = NSI_hist.mean(axis=0)-NSI_hist.std(axis=0), NSI_hist.mean(axis=0)+NSI_hist.std(axis=0)\n",
    "lb[lb<minc]=minc\n",
    "hb[hb<minc]=minc\n",
    "ax.fill_between(x, lb, hb, color='darkblue', alpha=.3)\n",
    "ax.set_ylabel('norm. log count')\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('NSI$_{pLFP}$ ($\\mu$V)')\n",
    "ax.set_xticks([-30,0,30,60])\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "fig.savefig('../doc/V1-NSI-log-hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "825ae1d5-94e4-4f05-a72b-1ba2b5871091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare to S1, loading a reduced dataset per cell\n",
    "\n",
    "NSI_bins_S1 = np.linspace(-15, 20, 50)\n",
    "\n",
    "if os.path.isfile('reduced_data/reduced_data_cell_1.npz'):\n",
    "    # if you have the S1 data, let's do the same on S1\n",
    "    NSI_hist_S1 = np.zeros((14, len(NSI_bins_S1)-1))\n",
    "    for i in np.arange(1, 15):\n",
    "        data = dict(np.load('reduced_data/reduced_data_cell_'+str(i)+'.npz'))\n",
    "        NSI_hist_S1[i-1,:] = np.histogram(data['NSI'][data['NSI_validated']], bins=NSI_bins_S1, density=True)[0]\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, figsize=(2.3,2.3))\n",
    "    x = .5*(NSI_bins_S1[1:]+NSI_bins_S1[:-1])\n",
    "    for i in range(NSI_hist_S1.shape[0]):\n",
    "        ax.plot(x, NSI_hist_S1[i,:], '-', lw=0.2, color='grey')\n",
    "\n",
    "    ax.annotate('quietly-sitting\\nS1', (0,1), xycoords='axes fraction', color='darkgreen', va='top')\n",
    "    ax.annotate('n=14 rec', (1,1), xycoords='axes fraction', color='darkgreen', va='top', ha='right', rotation=90, size='small')\n",
    "    ax.plot(x, NSI_hist_S1.mean(axis=0), lw=2, color='darkgreen')\n",
    "    ax.fill_between(x, NSI_hist_S1.mean(axis=0)-NSI_hist_S1.std(axis=0), NSI_hist_S1.mean(axis=0)+NSI_hist_S1.std(axis=0), color='darkgreen', alpha=.3)\n",
    "    ax.set_ylabel('norm. count')\n",
    "    ax.set_xlabel('NSI$_{pLFP}$ ($\\mu$V)')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([-10,0,10,20])   \n",
    "    plt.tight_layout()\n",
    "    fig.savefig('../doc/S1-NSI-hist.png')\n",
    "else:\n",
    "    print(' S1 data not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a5a4e-90b4-4109-949a-b9f24f97404b",
   "metadata": {},
   "source": [
    "## Studying the population rate correlate of NSI$_{pLFP}$-based network states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5decb-a688-4cba-86d2-a22c93c3ea6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71753051-f08f-40db-a1f1-6ce13fbf72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = np.linspace(1.5, 6, 10)\n",
    "\n",
    "output = np.zeros((len(sessions), len(alpha_values)))\n",
    "for i in range(len(sessions)):\n",
    "    #print('session', i)\n",
    "    data = Data(session_index=i, reduced=True)\n",
    "    data.compute_NSI(quantity='pop_act', alpha=2, verbose=False)\n",
    "    for a, alpha in enumerate(alpha_values):\n",
    "        data.compute_NSI(quantity='pLFP',\n",
    "                         alpha=alpha,\n",
    "                         p0_percentile=1,\n",
    "                         verbose=False)\n",
    "        data.validate_NSI(quantity='pLFP', verbose=False)\n",
    "        pop_act_low_freq_env = resample_trace(data.t_pop_act, data.pop_act_low_freq_env, data.t_pLFP)\n",
    "        delta_cond_validated = data.i_pLFP_vNSI & (data.pLFP_NSI<0)\n",
    "        output[i, a] = np.mean(pop_act_low_freq_env[delta_cond_validated])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49128b6-15c3-427e-84bf-1f62d47229da",
   "metadata": {},
   "source": [
    "## We plot sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a82c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_data(data, title='',\n",
    "                     time_points=[100, 1000],\n",
    "                     duration=2):\n",
    "\n",
    "\n",
    "    fig, AX_full = plt.subplots(6,len(time_points), figsize=(2.3*len(time_points), 7))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    if len(time_points)==1:\n",
    "        AX_full = [AX_full]\n",
    "        \n",
    "    YLIMS = [[np.inf, -np.inf] for i in range(len(AX_full))]\n",
    "    for t0, AX in zip(time_points, AX_full.T):\n",
    "        \n",
    "        # raster plot\n",
    "        if hasattr(data, 'V1_RASTER'):\n",
    "            for i, spikes in enumerate(data.V1_RASTER):\n",
    "                cond = (spikes>t0) & (spikes<(t0+duration))\n",
    "                AX[0].plot(spikes[cond], i+0*spikes[cond], 'o', ms=0.4, color='darkblue')\n",
    "                \n",
    "        AX[0].set_ylabel('units')\n",
    "        \n",
    "        # pop act. plot\n",
    "        data.plot('pop_act', t0=t0, duration=duration, ax=AX[1], color='darkblue')\n",
    "        data.plot('pop_act_sliding_mean', t0=t0, duration=duration, ax=AX[1], color='darkblue', lw=5, alpha=.2)\n",
    "        \n",
    "        # LFP plot\n",
    "        data.plot('LFP', t0=t0, duration=duration, ax=AX[2], color='dimgrey')\n",
    "        \n",
    "        # pLFP plot\n",
    "        data.plot('pLFP', t0=t0, duration=duration, ax=AX[3], color=plt.cm.tab10(5))\n",
    "        data.plot('pLFP_sliding_mean', t0=t0, duration=duration, ax=AX[3], color=plt.cm.tab10(5), lw=5, alpha=.2)\n",
    "\n",
    "        # NSI plot\n",
    "        data.plot('pLFP_NSI', t0=t0, duration=duration, ax=AX[4], color='k', lw=1)\n",
    "        data.plot('pLFP_vNSI', t0=t0, duration=duration, ax=AX[4], color=plt.cm.tab10(5), lw=0, ms=5)\n",
    "\n",
    "        # speed plot\n",
    "        data.plot('running_speed', t0=t0, duration=duration, ax=AX[5])\n",
    "\n",
    "        # labelling axes and setting the same limes\n",
    "        for j, label, ax in zip(range(len(AX)), \n",
    "                                ['units', 'rate (Hz)', 'LFP (mV)', '  pLFP ($\\mu$V)', 'NSI$_{pLFP}$ ($\\mu$V)', 'run. speed\\n (cm/s)'],\n",
    "                                AX):\n",
    "            if ax in AX_full.T[0]:\n",
    "                ax.set_ylabel(label)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_xlim([t0,t0+duration])\n",
    "            YLIMS[j] = [np.min([AX[j].get_ylim()[0], YLIMS[j][0]]),\n",
    "                        np.max([AX[j].get_ylim()[1], YLIMS[j][1]])]\n",
    "        AX[0].set_title('    $t_0$=%.1fs' % (t0-data.t0), size='small')\n",
    "        \n",
    "    for AX in AX_full.T:\n",
    "        for j in range(len(AX)):\n",
    "            ylim = [YLIMS[j][0]-.05*(YLIMS[j][1]-YLIMS[j][0]),\n",
    "                    YLIMS[j][1]+.05*(YLIMS[j][1]-YLIMS[j][0])]\n",
    "            AX[j].set_ylim(ylim)\n",
    "\n",
    "    for t0, AX in zip(time_points, AX_full.T):\n",
    "        AX[0].plot([t0,t0+0.2], (YLIMS[0][1])*np.ones(2), 'k-', lw=1)\n",
    "        AX[0].annotate('200ms', (0,1), xycoords='axes fraction')# (t0, YLIMS[0][1]))\n",
    "    \n",
    "    return fig, AX\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "eedea21f-b9a8-4a30-a931-85e0a5943246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - computing NSI for \"pLFP\" [...]\n",
      " - - > done !\n",
      " - computing NSI for \"pop_act\" [...]\n",
      " - - > done !\n",
      " - validating NSI for \"pLFP\" [...]\n",
      " - - > done !\n"
     ]
    }
   ],
   "source": [
    "data = Data(session_index=5, demo=False, reduced=True)\n",
    "            \n",
    "data.compute_NSI(quantity='pLFP',\n",
    "                 alpha=3.2,\n",
    "                 p0_percentile=1)\n",
    "data.compute_NSI(quantity='pop_act',\n",
    "                 alpha=2.,\n",
    "                 p0_percentile=0)\n",
    "data.validate_NSI(quantity='pLFP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "123cbdd3-50be-4f76-a3d5-17f8aa28a026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7026.89042951, 7027.08962955, 7040.23683261, 7055.77443622,\n",
       "       7094.61844526, 7095.01684535, 7813.73061247, 7900.38263262,\n",
       "       7945.40184309])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def look_for_good_trials(data, nsi_plfp_level,\n",
    "                         nsi_plfp_tolerance=3,\n",
    "                         rate_tolerance=2,\n",
    "                         with_running=False, \n",
    "                         run_thresh=3):\n",
    "    \n",
    "    pop_act_NSI_resampled = resample_trace(data.t_pop_act, \n",
    "                                           data.pop_act_NSI, data.t_pLFP)\n",
    "    \n",
    "\n",
    "    x, y = data.pLFP_NSI[data.i_pLFP_vNSI], pop_act_NSI_resampled[data.i_pLFP_vNSI]\n",
    "    cond = ((x>0) & (y>0)) | ((x<0) & (y<0))\n",
    "\n",
    "    lin = np.polyfit(x[cond], y[cond], 1)\n",
    "    \n",
    "    accuracy_cond = (np.abs(y-np.polyval(lin, x))<rate_tolerance) & \\\n",
    "        (np.abs(x-nsi_plfp_level)<nsi_plfp_tolerance)\n",
    "    \n",
    "    if with_running:\n",
    "        times = []\n",
    "        for t in data.t_pLFP[data.i_pLFP_vNSI][accuracy_cond]:\n",
    "            if data.t_running_speed[np.argmin((t-data.t_running_speed)**2)]>run_thresh:\n",
    "                times.append(t)\n",
    "        return times\n",
    "    else:\n",
    "        return data.t_pLFP[data.i_pLFP_vNSI][accuracy_cond]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c4f3-7984-4143-8e80-4fc1387be8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(session_index=1, demo=False, reduced=True)\n",
    "            \n",
    "data.compute_NSI(quantity='pLFP',\n",
    "                 alpha=3.2,\n",
    "                 p0_percentile=1)\n",
    "data.compute_NSI(quantity='pop_act',\n",
    "                 alpha=1.8,\n",
    "                 p0_percentile=0)\n",
    "data.validate_NSI(quantity='pLFP')\n",
    "\n",
    "get_accuracy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "df9c2d76-4519-479b-96f4-6e774be1087e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6956.37361311, 6956.57281316, 6967.13041561, 7052.18883539,\n",
       "       7056.17283632, 7094.41924521, 7095.61444549, 7542.02174929,\n",
       "       7683.85218227, 7684.05138232, 7815.32421284])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_for_good_trials(data, -25, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56b8f4-a135-4cd2-b2b6-a6cc4cb759a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d \n",
    "\n",
    "def get_accuracy(data,\n",
    "                 rate_tolerance=3,\n",
    "                 with_fig=True):\n",
    "    \n",
    "    pop_act_NSI_resampled = resample_trace(data.t_pop_act, \n",
    "                                           data.pop_act_NSI, data.t_pLFP)\n",
    "    \n",
    "\n",
    "    x, y = data.pLFP_NSI[data.i_pLFP_vNSI], pop_act_NSI_resampled[data.i_pLFP_vNSI]\n",
    "    cond = ((x>0) & (y>0)) | ((x<0) & (y<0))\n",
    "\n",
    "    lin = np.polyfit(x[cond], y[cond], 1)\n",
    "    \n",
    "    accuracy_cond = np.abs(y-np.polyval(lin, x))<rate_tolerance\n",
    "    \n",
    "    accuracy = 100*np.sum(accuracy_cond)/len(y)    \n",
    "    if with_fig:\n",
    "        fig, ax = plt.subplots(1, figsize=(2,2))\n",
    "        ax.set_title('accuracy=%.1f%%'%accuracy, fontsize=11)\n",
    "        x = np.linspace(x.min(), x.max())\n",
    "        ax.plot(data.pLFP_NSI[data.i_pLFP_vNSI], \n",
    "                 pop_act_NSI_resampled[data.i_pLFP_vNSI], 'o', lw=1, ms=0.2)\n",
    "        ax.fill_between(x, np.polyval(lin, x)-rate_tolerance, np.polyval(lin, x)+rate_tolerance, color='g', alpha=.3)\n",
    "        ax.plot(x, np.polyval(lin, x), 'k-')\n",
    "        ax.set_ylabel('NSI$_{\\,rate}$ (Hz)')\n",
    "        ax.set_xlabel('NSI$_{\\,pLFP}$ ($\\mu$V)')\n",
    "        return fig, ax, accuracy\n",
    "    else:\n",
    "        return accuracy\n",
    "    \n",
    "get_accuracy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16ffd31a-27a6-4a11-9601-9a2cde266b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059509277 5e-05\n",
      "0.078430176 5e-05\n",
      "0.14312744 4.999999998744897e-05\n",
      "0.18432617 4.999999998744897e-05\n",
      "0.14404297 4.999999998744897e-05\n",
      "0.06652832 5.000000000165983e-05\n",
      "0.066833496 5.000000000165983e-05\n",
      "0.18981934 5.000000000165983e-05\n",
      "0.3338623 5.000000000165983e-05\n",
      "0.2154541 5e-05\n",
      "-0.25115967 5e-05\n",
      "-0.025024414 5e-05\n",
      "-0.021057129 5e-05\n",
      "0.30670166 5e-05\n"
     ]
    }
   ],
   "source": [
    "#%run exp_data.py\n",
    "# loading a reduced dataset per cell\n",
    "DATA = []\n",
    "for i in np.arange(1, 15):\n",
    "    data = dict(np.load('reduced_data/reduced_data_cell_'+str(i)+'.npz'))\n",
    "    print(data['sbsmpl_Extra'][0], data['dt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb2737-5cbf-46d3-ac8c-cc9067023951",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = dict(np.load('cortical_arousal_index/data/final_alpha.npz'))\n",
    "fig_lf, ax2 = figure(figsize=(.75, 1.), top=1.5)\n",
    "mean, std, alpha = [], [], []\n",
    "for i in range(OUTPUT['VM_LOW_FREQ_POWER'].shape[0])[3::2]:\n",
    "    cond = np.isfinite(OUTPUT['VM_LOW_FREQ_POWER'][i,:]) & (OUTPUT['N_LOW_FREQ'][i,:]>10)\n",
    "    mean.append(np.mean(OUTPUT['VM_LOW_FREQ_POWER'][i,:][cond]))\n",
    "    std.append(np.std(OUTPUT['VM_LOW_FREQ_POWER'][i,:][cond]))\n",
    "    alpha.append(OUTPUT['ALPHA'][i])\n",
    "ylim = [4.1,10.5]\n",
    "color=Purple    \n",
    "ax2.plot(alpha, np.array(mean), color=Purple, lw=3)\n",
    "ax2.fill_between(alpha, np.array(mean)+np.array(std),\n",
    "                 np.array(mean)-np.array(std), color=Purple, alpha=.3, lw=0)\n",
    "# exponential fit\n",
    "def exp(x, p):\n",
    "    return p[2]+p[1]*np.exp(-np.array(x)/p[0])\n",
    "def to_minimize(p):\n",
    "    return np.sum(np.abs(mean-exp(np.array(alpha)-alpha[0], p)))\n",
    "from scipy.optimize import minimize\n",
    "plsq = minimize(to_minimize, [2., 5., 5.])\n",
    "ax2.plot(np.ones(2)*(alpha[0]+plsq.x[0]), [ylim[0], ylim[1]-1], '--', color=Brown, lw=.5)\n",
    "ax2.annotate('$\\\\alpha^{opt}$', (alpha[0]+plsq.x[0], ylim[1]-1), color=Brown, fontsize=FONTSIZE)\n",
    "ax2.plot((alpha[0], alpha[0]+plsq.x[0], alpha[-1]), [mean[0], plsq.x[2], plsq.x[2]], '--', color=Brown, lw=.5)\n",
    "ax2.plot(alpha, exp(alpha-alpha[0], plsq.x), '--', color='k', lw=1, label='exp. fit')\n",
    "ax2.legend(loc=(-.1,1.05), frameon=False, prop={'size':FONTSIZE-1}, handlelength=1.8, handletextpad=0.5)\n",
    "set_plot(ax2, xlabel='$\\\\alpha$ (unitless)', ylabel='$V_m$ $\\delta_{env}$ (mV)   ', ylim=ylim)\n",
    "alpha0 = alpha[0]+plsq.x[0]\n",
    "print(round(alpha0,2))\n",
    "update_study_output('alpha_opt', str(round(alpha0,2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
